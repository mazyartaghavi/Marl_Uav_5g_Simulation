# pymarl/src/envs/UAVEnv.py

import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from envs.multiagentenv import MultiAgentEnv
from scipy.optimize import minimize
import random

class UAVEnv(MultiAgentEnv):
    def __init__(self, **kwargs):
        self.n_agents = kwargs.get("n_agents", 10)
        self.map_size = kwargs.get("map_size", [100, 100])
        self.max_episode_length = kwargs.get("max_episode_length", 100)
        self.episode_limit = self.max_episode_length
        self.kernel = C(1.0) * RBF(10.0)
        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=5)
        self.mc_dropout_samples = 10
        self.use_thompson = kwargs.get("use_thompson", False)
        self.use_entropy_acq = kwargs.get("use_entropy_acq", False)
        self.vis_save_path = kwargs.get("vis_save_path", "trajectory.png")
        self.reset()

    def reset(self):
        self.t = 0
        self.agent_pos = np.random.rand(self.n_agents, 2) * self.map_size
        self.trajectory = [[] for _ in range(self.n_agents)]
        self.signal_sources = np.random.rand(10, 2) * self.map_size
        self.signal_strengths = np.random.rand(10) * 100
        self.train_gp()
        return self.get_obs(), self.get_state()

    def step(self, actions):
        rewards = []
        terminated = False
        for i, action in enumerate(actions):
            best_move = self.bayesian_optimization_move(self.agent_pos[i])
            self.agent_pos[i] += best_move
            self.agent_pos[i] = np.clip(self.agent_pos[i], 0, self.map_size)
            self.trajectory[i].append(self.agent_pos[i].copy())
            rewards.append(self._calculate_signal_gain(self.agent_pos[i]))

        self.t += 1
        if self.t >= self.episode_limit:
            terminated = True
            self.visualize_trajectories()
            self.visualize_signal_heatmap()

        return rewards, terminated, {}

    def _action_to_movement(self, action):
        direction_map = {
            0: np.array([0, 1]),
            1: np.array([1, 0]),
            2: np.array([0, -1]),
            3: np.array([-1, 0]),
            4: np.array([0, 0]),
        }
        return direction_map.get(action, np.array([0, 0]))

    def _calculate_signal_gain(self, pos):
        pos = np.array(pos).reshape(1, -1)
        preds = [self.gp.predict(pos)[0] for _ in range(self.mc_dropout_samples)]
        return np.mean(preds)

    def train_gp(self):
        X = self.signal_sources
        y = self.signal_strengths
        self.gp.fit(X, y)

    def bayesian_optimization_move(self, current_pos):
        def ucb_acquisition(x):
            x = np.array(x).reshape(1, -1)
            preds = [self.gp.predict(x)[0] for _ in range(self.mc_dropout_samples)]
            mean = np.mean(preds)
            std = np.std(preds)
            return -(mean + 0.5 * std)  # UCB

        def thompson_acquisition(x):
            x = np.array(x).reshape(1, -1)
            return -self.gp.predict(x)[0] + np.random.normal(0, 5)

        def entropy_acquisition(x):
            x = np.array(x).reshape(1, -1)
            preds = [self.gp.predict(x)[0] for _ in range(self.mc_dropout_samples)]
            return -np.std(preds)

        acquisition = ucb_acquisition
        if self.use_thompson:
            acquisition = thompson_acquisition
        elif self.use_entropy_acq:
            acquisition = entropy_acquisition

        res = minimize(acquisition, current_pos, bounds=[(0, self.map_size[0]), (0, self.map_size[1])])
        direction = res.x - current_pos
        direction = np.clip(direction, -1, 1)
        return direction

    def get_obs(self):
        return [np.concatenate([self.agent_pos[i], [self._calculate_signal_gain(self.agent_pos[i])]]) for i in range(self.n_agents)]

    def get_state(self):
        return self.agent_pos.flatten()

    def get_obs_size(self):
        return 3

    def get_state_size(self):
        return self.n_agents * 2

    def get_avail_actions(self):
        return [[1] * 5 for _ in range(self.n_agents)]

    def get_total_actions(self):
        return 5

    def get_env_info(self):
        return {
            "n_actions": self.get_total_actions(),
            "n_agents": self.n_agents,
            "state_shape": self.get_state_size(),
            "obs_shape": self.get_obs_size(),
            "episode_limit": self.episode_limit,
        }

    def visualize_trajectories(self):
        plt.figure(figsize=(10, 10))
        for i, traj in enumerate(self.trajectory):
            if traj:
                traj = np.array(traj)
                plt.plot(traj[:, 0], traj[:, 1], label=f'UAV {i}')
        plt.xlim(0, self.map_size[0])
        plt.ylim(0, self.map_size[1])
        plt.title("UAV Trajectories")
        plt.legend()
        plt.grid(True)
        plt.savefig(self.vis_save_path)
        plt.close()

    def visualize_signal_heatmap(self):
        xx, yy = np.meshgrid(np.linspace(0, self.map_size[0], 100), np.linspace(0, self.map_size[1], 100))
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = np.array([np.mean([self.gp.predict([pt])[0] for _ in range(self.mc_dropout_samples)]) for pt in grid_points])
        Z = Z.reshape(xx.shape)
        plt.figure(figsize=(10, 10))
        plt.contourf(xx, yy, Z, cmap='viridis')
        plt.colorbar(label='Signal Strength')
        plt.title("5G Signal Strength Map")
        plt.scatter(self.signal_sources[:, 0], self.signal_sources[:, 1], color='red', label='Signal Sources')
        plt.legend()
        plt.savefig("signal_heatmap.png")
        plt.close()
